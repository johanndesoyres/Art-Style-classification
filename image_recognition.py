# -*- coding: utf-8 -*-
"""Image_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/127deDIjrYmegb-52HkkPSk53NCmwxsss
"""

!pip install --upgrade tensorflow

from __future__ import absolute_import, division, print_function, unicode_literals
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""Home-made Classes and Functions"""

import matplotlib.pyplot as plt
import numpy as np

############ Classes ##############

class Data:
     def __init__(self, image, label):
        self.image = image
        self.label = label

class _Deformer(object):
    def getmesh(self, im):
        x, y = im.size
        return [(
            # target rectangle
            (0, 0, x, y),
            # corresponding source quadrilateral (1)
            # (NW, SW, SE, and NE. see method=QUAD)
            (0, y, 0, 0, x, y, x, 0))]

############ Functions ##############

def show_images(images, cols = 1, titles = None):
    """Display a list of images in a single figure with matplotlib.
    
    Parameters
    ---------
    images: List of np.arrays compatible with plt.imshow.
    
    cols (Default = 1): Number of columns in figure (number of rows is 
                        set to np.ceil(n_images/float(cols))).
    
    titles: List of titles corresponding to each image. Must have
            the same length as titles.
    """
    assert((titles is None)or (len(images) == len(titles)))
    n_images = len(images)
    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]
    fig = plt.figure()
    for n, (image, title) in enumerate(zip(images, titles)):
        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)
        if image.ndim == 2:
            plt.gray()
        plt.imshow(image)
        a.set_title(title)
    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)
    plt.show()

"""Pré-traitements des images"""

import os
import numpy as np
from collections import Counter
import pandas as pd

styles = os.listdir("drive/My Drive/peintures")#liste les styles de peintures du dataset
occu = []

for style in styles:
  peintures = os.listdir("drive/My Drive/peintures/"+style)#liste toutes les peintures d'un style particulier
  for peinture in peintures:
    occu.append(style)

styles_count = Counter(occu)#nombre de tableaux pour chaque style

df = pd.DataFrame.from_dict(styles_count, orient='index',columns=['Frequency'])
df = df.sort_values(by=['Frequency'])
df.plot.barh()#Affiche la repartition des peintures a travers les differents styles

from PIL import Image
from PIL import ImageOps
from PIL import ImageFilter
import matplotlib.pyplot as plt

images = []
new_images = []
im = Image.open("drive/My Drive/peintures/baroque/Caravaggio_30.jpg").convert('RGB').resize((150,150), Image.ANTIALIAS)

images.append(im)
images.append(ImageOps.flip(im))#on retourne l'image
images.append(ImageOps.mirror(im))#effet miroir
images.append(ImageOps.deform(im, deformer=_Deformer()))#distortion de l'image en utilisant un objet gitmesh
images.append(im.filter(ImageFilter.BLUR))#on floute l'image

for image in images:
  new_images.append(np.array(image))

show_images(new_images,titles=['Original','Flip horizontal','Effet miroir','Distortion','Floutage'])

from PIL import Image
from PIL import ImageOps
from PIL import ImageFilter
import numpy as np
import os
import random

styles = os.listdir("drive/My Drive/peintures")#styles de peintures du dataset
dataset1 = []#images sans couleurs
dataset2 = []#images avec couleurs (RGB)
labels = []#reponse attendue pour l'image

for style in styles:
  peintures = os.listdir("drive/My Drive/peintures/"+style)
  peintures = random.choices(peintures, k=80)#choix au hasard de 20 peintures dans le repertoire
  for peinture in peintures:
    try:
      file_name = "drive/My Drive/peintures/"+style+"/"+peinture
      img2 = Image.open(file_name).convert('RGB').resize((150,150), Image.ANTIALIAS)#on formate l'image dans une taille standard et petit
      img1 = ImageOps.grayscale(img2)

      dataset1.append(np.array(img1)) # Ajout de l'image sans couleurs en format numpy.array dans dataset
      dataset1.append(np.array(ImageOps.flip(img1)))
      dataset1.append(np.array(ImageOps.mirror(img1)))
      dataset1.append(np.array(ImageOps.deform(img1, deformer=_Deformer())))
      dataset1.append(np.array(img1.filter(ImageFilter.BLUR)))

      dataset2.append(np.array(img2)) # Ajout de l'image avec couleurs en format numpy.array dans dataset
      dataset2.append(np.array(ImageOps.flip(img2)))
      dataset2.append(np.array(ImageOps.mirror(img2)))
      dataset2.append(np.array(ImageOps.deform(img2, deformer=_Deformer())))
      dataset2.append(np.array(img2.filter(ImageFilter.BLUR)))

      if(style=="baroque"):#ajout de la reponse attendue dans labels
        labels.extend([0,0,0,0,0])
      elif(style=="cubisme"):
        labels.extend([1,1,1,1,1])
      elif(style=="fauvisme"):
        labels.extend([2,2,2,2,2])
      elif(style=="gothique"):
        labels.extend([3,3,3,3,3])
      elif(style=="impressionisme"):
        labels.extend([4,4,4,4,4])
      elif(style=="moderne"):
        labels.extend([5,5,5,5,5])
      elif(style=="postimpressionisme"):
        labels.extend([6,6,6,6,6])
      elif(style=="realisme"):
        labels.extend([7,7,7,7,7])
      elif(style=="renaissance"):
        labels.extend([8,8,8,8,8])
      elif(style=="romantisme"):
        labels.extend([9,9,9,9,9])
      elif(style=="surrealisme"):
        labels.extend([10,10,10,10,10])
      elif(style=="symbolisme"):
        labels.extend([11,11,11,11,11]) 
    except OSError:
      print("Can not open file " + file_name)

import numpy as np

FinalData1=[]
FinalData2=[]

for i in range (len(dataset1)):
  FinalData1.append(Data(dataset1[i],labels[i]))  
  FinalData2.append(Data(dataset2[i],labels[i]))

"""Normalization des images"""

import numpy as np
#Normalization
#Inew = (I - I.mean) / I.std
for i in range (len(FinalData2)):
  FinalData2[i].image = (FinalData2[i].image - np.mean(FinalData2[i].image))/np.std(FinalData2[i].image)

"""Visualisation finale d'un echantillon"""

import matplotlib.pyplot as plt
import numpy as np

#visualisation d'une image du dataset 1 et du dataset 2
print("######################  Dataset 1 (images sans couleurs) ######################")
show_images([FinalData1[20].image,FinalData1[21].image,FinalData1[22].image,FinalData1[23].image,FinalData1[24].image])
#print("######################  Dataset 2 (images avec couleurs) ######################")
#show_images([FinalData2[20].image,FinalData2[21].image,FinalData2[22].image,FinalData2[23].image,FinalData2[24].image])

"""RNN Classiques"""

#Construction du train set et du test set
from sklearn.model_selection import train_test_split
import numpy as np

train , test = train_test_split(FinalData1,test_size=0.25)

train_x = [data.image for data in train]
train_y = [data.label for data in train]
test_x = [data.image for data in test]
test_y = [data.label for data in test]

trainx=np.asarray(train_x)
testx=np.asarray(test_x)
trainy=np.asarray(train_y)
testy=np.asarray(test_y)

#On convertit les jeux de donnees en format tf.tensor pour que le format des donnees soit le plus adapte à notre RNN.
trainx = tf.convert_to_tensor(train_x, dtype=tf.float32)
testx = tf.convert_to_tensor(test_x, dtype=tf.float32)

import pandas as pd
import numpy as np

Results = pd.DataFrame([[2,0,0,0],[3,0,0,0],[4,0,0,0]],
                       index=["Model 1","Model 2","Model 3"],
                       columns=["nb hidden layers","nb weights","test loss","test acc"],
                       dtype=np.float128)

for i in range(0,3):
  if(i==0):
    #construction du model
    model1 = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(150,150)),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='elu'),
    tf.keras.layers.Dense(12, activation='softmax')])
  elif(i==1):
    #construction du model
    model1 = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(150,150)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='elu'),
    tf.keras.layers.Dense(12, activation='softmax')
    ])
  else:
    #construction du model
    model1 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(150,150)),
    tf.keras.layers.Dense(784, activation='relu'),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='elu'),
    tf.keras.layers.Dense(12, activation='softmax')
    ])
  #Definition des parametres d'apprentissage du RNN
  model1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
  #Apprentissage
  model1.fit(trainx, trainy, epochs=15)
  #evaluation du modele
  test_loss, test_acc = model1.evaluate(testx,  testy, verbose=2)
  Results["test loss"][i]=test_loss
  Results["test acc"][i]=test_acc
  Results["nb weights"][i]=model1.count_params()
  del model1

print("______________________________________________________________________________________________")
print("\nResults\n\n")
Results

#On remarque qu'en augmentant le nombe de layers et donc le nombre de poids l'accuracy du RNN varie mais ne diminue pas forcement.
#Ici nous avons fixe le nombre de hidden layers et de neurones pour chacune de ces layers un peu au hasard.
#Il faudrait donc tester plein de models avec chacun un nombre de hidden layers et neurones different et esperer tomber sur la meilleure combinaison.
#Cette appproche n'est pas efficace et elle ne garantie même pas un resultat convenable. On va donc tester une autre approche.

"""RNN Convolutif"""

#Construction du train set et du test set
from sklearn.model_selection import train_test_split
import numpy as np

train2 , test2 = train_test_split(FinalData2,test_size=0.25)

train_x2 = [data.image for data in train2]
train_y2 = [data.label for data in train2]
test_x2 = [data.image for data in test2]
test_y2 = [data.label for data in test2]

trainx2=np.asarray(train_x2)
testx2=np.asarray(test_x2)
trainy2=np.asarray(train_y2)
testy2=np.asarray(test_y2)

#On convertit les jeux de donnees en format tf.tensor pour que le format des donnees soit le plus adapte à notre RNN.
trainx2 = tf.convert_to_tensor(trainx2, dtype=tf.float32)
testx2 = tf.convert_to_tensor(testx2, dtype=tf.float32)

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten

#construction d'un RNN convolutif
model2 = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(12, activation='softmax')
])

#choix de l'algo d'apprentissage
model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
#Apprentissage du modele
model2.fit(trainx2, trainy2, epochs=7,steps_per_epoch=len(trainx2))
#evaluation du modele
test_loss2, test_acc2 = model2.evaluate(testx2,  testy2, verbose=2)
print('\nTest accuracy:', test_acc2)

#Ici on se retrouve avec le meme probleme qu'un RNN classique. Il est necessaire de tester beaucoup de modeles en faisant varier le nombre de hidden layers,
#le nombre et le type de couche de convolution. On peut alors esperer tomber sur mla meilleure combianaison possible mais ce n'est âs garantie et la compilation de tous ces modeles
#prendrai enormement de temps. On va donc utiliser des modeles pre-entraines.

"""RNN with Transfer Learning **(VGG19)**"""

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten

#Import du modele VGG19 pre-entrainer sur le dataset inagenet
imagenet_model = tf.keras.applications.VGG19(weights = "imagenet", 
                           include_top=False, 
                           input_shape = (150, 150, 3),
                           pooling='max')

#Couches du modele VGG19
imagenet_model.layers

# freeze feature layers and rebuild model
for l in imagenet_model.layers:
    l.trainable = False

#construction du model
model3 = [tf.keras.layers.Flatten(),
          tf.keras.layers.Dense(128, activation='relu'),
          tf.keras.layers.Dense(12, activation='softmax')]

#Jointure des deux modeles
model_using_pre_trained_one = tf.keras.Sequential( imagenet_model.layers + model3 )

#choix de l'algo d'apprentissage
model_using_pre_trained_one.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

#Apprentissage du modele
model_using_pre_trained_one.fit(trainx2, trainy2, epochs=7,steps_per_epoch=len(trainx2))

#evaluation du modele
test_loss3, test_acc3 = model_using_pre_trained_one.evaluate(testx2,  testy2, verbose=2)
print('\nTest accuracy:', test_acc3)
print('\nTest loss:', test_loss3)

"""RNN with Transfer Learning **(VGG16)**"""

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten

#Import du modele VGG16 pre-entrainer sur le dataset inagenet
imagenet_model2 = tf.keras.applications.VGG16(weights = "imagenet", 
                           include_top=False, 
                           input_shape = (150, 150, 3),
                           pooling='max')
#Couches du modele VGG16
imagenet_model2.layers

# freeze feature layers and rebuild model
for l in imagenet_model2.layers:
    l.trainable = False

#construction du model
model4 = [
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(12, activation='softmax')
]

#Jointure des deux modeles
model_using_pre_trained_one2 = tf.keras.Sequential( imagenet_model2.layers + model4 )

#choix de l'algo d'apprentissage
model_using_pre_trained_one2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

#Apprentissage du modele
model_using_pre_trained_one2.fit(trainx2, trainy2, epochs=7,steps_per_epoch=len(trainx2))

#evaluation du modele
test_loss4, test_acc4 = model_using_pre_trained_one2.evaluate(testx2,  testy2, verbose=2)
print('\nTest accuracy:', test_acc4)